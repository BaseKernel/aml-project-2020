{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svcca(x, y, cutoff=0.99):\n",
    "    \n",
    "    u_x, s_x, v_x = torch.svd(x - x.mean(axis=0))\n",
    "    u_y, s_y, v_y = torch.svd(y - y.mean(axis=0))\n",
    "    \n",
    "    var_x = torch.cumsum(s_x**2/torch.sum(s_x**2), dim=0)\n",
    "    var_y = torch.cumsum(s_y**2/torch.sum(s_y**2), dim=0)\n",
    "    \n",
    "    u_1 = u_x[:,:torch.where(var_x > cutoff)[0][0]+1]\n",
    "    u_2 = u_y[:,:torch.where(var_y > cutoff)[0][0]+1]\n",
    "    \n",
    "    s_1 = s_x[:torch.where(var_x > cutoff)[0][0]+1]\n",
    "    s_2 = s_y[:torch.where(var_y > cutoff)[0][0]+1]\n",
    "    \n",
    "    v_1 = v_x[:,:torch.where(var_x > cutoff)[0][0]+1]\n",
    "    v_2 = v_y[:,:torch.where(var_y > cutoff)[0][0]+1]\n",
    "    \n",
    "    uu = u_1.t() @ u_2\n",
    "    u, diag, v = torch.svd(uu)\n",
    "    a = (v_1 * s_1.reciprocal_().unsqueeze_(0)) @ u\n",
    "    b = (v_2 * s_2.reciprocal_().unsqueeze_(0)) @ v\n",
    "    return a, b, diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineTask:\n",
    "    def __init__(self):\n",
    "        self.amplitude = np.random.uniform(0.1, 5.0)\n",
    "        self.phase = np.random.uniform(0, 2*np.pi)\n",
    "        self.hold_x = None\n",
    "        \n",
    "    def sin(self, x):\n",
    "        return self.amplitude * np.sin(x + self.phase)\n",
    "    \n",
    "    def training_data(self, n=10, fresh = False):\n",
    "        if self.hold_x is None:\n",
    "            self.hold_x = np.random.uniform(-5, 5, size=(n,1))\n",
    "            x = self.hold_x\n",
    "        else:\n",
    "            if fresh:\n",
    "                x = np.random.uniform(-5, 5, size=(n,1))\n",
    "            else:\n",
    "                x = self.hold_x\n",
    "        y = self.sin(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "            \n",
    "        \n",
    "    def test_data(self, n=50):\n",
    "        x = np.linspace(-5, 5, num=n).reshape(n,1)\n",
    "        y = self.sin(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    \n",
    "class SineModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(1, 40)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(40, 40)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(40, 1)\n",
    "        self.parameter_dict = {name: par for name, par in self.named_parameters()}\n",
    "        self.activations_relu1 = []\n",
    "        self.activations_relu2 = []\n",
    "        self.activations_layer3 = []\n",
    "        self.handles = []\n",
    "    \n",
    "    def forward(self, x, parameters=None):\n",
    "        if parameters is None:\n",
    "            x = self.layer1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.layer3(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = F.linear(x, parameters['layer1.weight'], parameters['layer1.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.linear(x, parameters['layer2.weight'], parameters['layer2.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.linear(x, parameters['layer3.weight'], parameters['layer3.bias'])\n",
    "            return x\n",
    "                \n",
    "    def get_parameter_dict(self):\n",
    "        self.update_parameter_dict()\n",
    "        return self.parameter_dict\n",
    "    \n",
    "    def get_activations(self, name):\n",
    "        return getattr(self, 'activations_'+name)\n",
    "    \n",
    "    def update_parameter_dict(self):\n",
    "        self.parameter_dict = {name: par for name, par in self.named_parameters()}\n",
    "\n",
    "    def copy(self, model):\n",
    "        self.load_state_dict(model.state_dict())\n",
    "        \n",
    "    def set_hooks(self):\n",
    "        \n",
    "        def hook_fn(name):\n",
    "            def hook(model, input, output):\n",
    "                getattr(self, 'activations_'+name).append(output.detach())\n",
    "            return hook\n",
    "            \n",
    "        for name, module in self.named_modules():\n",
    "            h = hook_fn(name)\n",
    "            if name[:-1] == 'relu' or name == 'layer3':\n",
    "                self.handles.append(getattr(self,name).register_forward_hook(h))\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "        self.reset_activations()\n",
    "    \n",
    "    def reset_activations(self):\n",
    "        for name, module in self.named_modules():\n",
    "            if name[:-1] == 'relu' or name == 'layer3':\n",
    "                getattr(self, 'activations_'+name).clear()\n",
    "\n",
    "        \n",
    "class MetaLearning:\n",
    "    def __init__(self):\n",
    "        self.meta_model = SineModel()\n",
    "        self.tuned_model = None\n",
    "        self.meta_parameters = self.meta_model.get_parameter_dict()\n",
    "        self.metric = torch.nn.MSELoss()\n",
    "        self.activation = []\n",
    "    \n",
    "    def get_parameters(self): # depreciated\n",
    "        self.update_meta_parameters()\n",
    "        return self.meta_parameters\n",
    "    \n",
    "    def get_exp_results(self):\n",
    "        return self.exp_results\n",
    "    \n",
    "    def update_meta_parameters(self):\n",
    "        self.meta_parameters = self.meta_model.get_parameter_dict()\n",
    "            \n",
    "    def train(self, epochs=100, num_tasks=25, num_grad_steps=1, meta_lr=1e-3, step_lr=0.01, foa=False, config='maml', exp=False):\n",
    "        self.epochs = epochs\n",
    "        self.num_tasks = num_tasks\n",
    "        self.meta_lr = meta_lr\n",
    "        self.step_lr = step_lr\n",
    "        self.foa = foa\n",
    "        self.config = config\n",
    "        self.optimizer = torch.optim.Adam(self.meta_model.parameters(), lr=self.meta_lr)\n",
    "        self.exp_results = []\n",
    "        \n",
    "        if exp:\n",
    "            self.exp_tasks = [SineTask() for m in range(10)]\n",
    "            \n",
    "        hold_exp = []\n",
    "        history = []\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            if exp:\n",
    "                if epoch % 10 == 0:\n",
    "                    hold_exp.append(self.exp_weight_distance())\n",
    "                \n",
    "            task_batch_losses = []\n",
    "            tasks = [SineTask() for m in range(self.num_tasks)]\n",
    "            for task in tasks:\n",
    "                \n",
    "                task_model = SineModel()\n",
    "                task_model.copy(self.meta_model)\n",
    "                hold_parameters = self.meta_model.get_parameter_dict()\n",
    "                for step in range(num_grad_steps):\n",
    "                    \n",
    "                    x, y = task.training_data()\n",
    "                    y_hat = task_model.forward(x, hold_parameters)\n",
    "                    loss = self.metric(y_hat, y)\n",
    "                    grads = self.get_gradients(loss, hold_parameters)\n",
    "                    for ((name, parameter), grad) in zip(hold_parameters.items(),grads):\n",
    "                        hold_parameters[name] = parameter - self.step_lr * grad\n",
    "                    \n",
    "                x, y = task.training_data(fresh=True)\n",
    "                y_hat = self.meta_model.forward(x, hold_parameters)\n",
    "                meta_loss = self.metric(y_hat, y)\n",
    "                meta_loss.backward(retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                task_batch_losses.append(meta_loss.item())\n",
    "                self.meta_model.update_parameter_dict()\n",
    "                \n",
    "            history.append(np.mean(task_batch_losses))\n",
    "        \n",
    "        # make result neat\n",
    "        if exp:\n",
    "            # make plot function\n",
    "            res1 = []\n",
    "            res2 = []\n",
    "            res3 = []\n",
    "            for exp in hold_exp:\n",
    "                res1.append(exp['layer1'])\n",
    "                res2.append(exp['layer2'])\n",
    "                res3.append(exp['layer3'])\n",
    "            self.exp_results = [np.array(res1),np.array(res2),np.array(res3)]\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def get_gradients(self, loss, parameters=None, fine_tune=False):\n",
    "        # maml\n",
    "        if self.config == 'maml':\n",
    "            if fine_tune:\n",
    "                grads = torch.autograd.grad(loss, self.tuned_model.parameters())\n",
    "            else:\n",
    "                grads = torch.autograd.grad(loss, parameters.values(), retain_graph=not self.foa)\n",
    "            return grads\n",
    "        # anil      \n",
    "        elif self.config == 'anil':\n",
    "            grads = []\n",
    "            if fine_tune:\n",
    "                for name, parameter in self.tuned_model.named_parameters():\n",
    "                    if 'layer3' in name:\n",
    "                        grads.append(torch.autograd.grad(loss, parameter, retain_graph=True)[0])\n",
    "                    else:\n",
    "                        grads.append(torch.zeros_like(parameter))\n",
    "            else:\n",
    "                for name, parameter in parameters.items():\n",
    "                    if 'layer3' in name:\n",
    "                        grads.append(torch.autograd.grad(loss, parameter, create_graph=not self.foa)[0])\n",
    "                    else:\n",
    "                        grads.append(torch.autograd.grad(loss, parameter, retain_graph=True)[0])\n",
    "            return tuple(grads)\n",
    "        \n",
    "    def get_custom_gradients(self, loss, parameters=None, frozen_layers=None):\n",
    "        grads = []\n",
    "        for name, parameter in self.tuned_model.get_parameter_dict().items(): #self.tuned_model.named_parameters():\n",
    "            if name.split('.')[0] in frozen_layers:\n",
    "                grads.append(torch.zeros_like(parameter))\n",
    "            else:\n",
    "                grads.append(torch.autograd.grad(loss, parameter, create_graph=not self.foa)[0])\n",
    "        return tuple(grads)\n",
    "                \n",
    "            \n",
    "    def fine_tune(self, task, num_examples=5, gradient_steps=10, frozen_layers=None):\n",
    "        self.tuned_model = SineModel()\n",
    "        self.tuned_model.copy(self.meta_model)\n",
    "        tune_history = []\n",
    "        \n",
    "        for step in range(gradient_steps):\n",
    "            x, y = task.training_data()\n",
    "            y_hat = self.tuned_model.forward(x)\n",
    "            loss = self.metric(y_hat, y)\n",
    "            if frozen_layers is None:\n",
    "                grads = self.get_gradients(loss, fine_tune=True)\n",
    "            else:\n",
    "                grads = self.get_custom_gradients(loss, frozen_layers=frozen_layers)\n",
    "            with torch.no_grad():\n",
    "                for parameter, grad in zip(self.tuned_model.parameters(), grads):\n",
    "                    parameter -= self.step_lr * grad\n",
    "            tune_history.append(loss.item())\n",
    "        return tune_history\n",
    "        \n",
    "    def freezing_layer_test():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def exp_weight_distance(self):\n",
    "        hold_values = {'layer1': [],\n",
    "                       'layer2': [],\n",
    "                       'layer3': []}\n",
    "        result = {}\n",
    "        for task in self.exp_tasks:\n",
    "            self.fine_tune(task, gradient_steps=10)\n",
    "            for ((name, par1), par2) in zip(self.tuned_model.named_parameters(), self.meta_model.parameters()):\n",
    "                \n",
    "                if name.split('.')[1] == 'weight':\n",
    "                    hold_values[name.split('.')[0]].append(torch.cdist(par1,par2).detach().numpy())\n",
    "        \n",
    "        #print(hold_values)\n",
    "        for name, value in hold_values.items():\n",
    "            result[name] = [np.mean(hold_values[name]),np.std(hold_values[name])]\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def exp_similarity_CCA(self, num_tasks=10, shots=10):\n",
    "        \n",
    "        tasks = [SineTask() for m in range(num_tasks)]\n",
    "        hold_values = {'relu1': [],\n",
    "                       'relu2': [],\n",
    "                       'layer3':[]}\n",
    "        for task in tasks:\n",
    "            self.fine_tune(task, num_examples=shots, gradient_steps = 10)\n",
    "            self.meta_model.set_hooks()\n",
    "            self.tuned_model.set_hooks()\n",
    "            \n",
    "            x, _ = task.training_data(n=400, fresh=True)\n",
    "            \n",
    "            for x_i in x:\n",
    "                self.meta_model(x_i)\n",
    "                self.tuned_model(x_i)\n",
    "                \n",
    "            for name, modeule in self.meta_model.named_modules():\n",
    "                if name[:-1] == 'relu' or name == 'layer3':\n",
    "                    meta_act = torch.stack(self.meta_model.get_activations(name))\n",
    "                    tuned_act = torch.stack(self.tuned_model.get_activations(name))\n",
    "                    _, _, diag = svcca(meta_act, tuned_act)\n",
    "                    hold_values[name].append(np.mean(diag.numpy()))\n",
    "            self.meta_model.remove_hooks()\n",
    "            self.tuned_model.remove_hooks()\n",
    "            \n",
    "        print(\"Over % 2d tasks\" % num_tasks)\n",
    "        for name, value in hold_values.items():\n",
    "            print(\"mean cca of {0}: {1:1.4f} std cca of {2}: {3:1.4f}\".format(name,np.mean(value),name,np.std(value)))\n",
    "     \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.15418267250061\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs=800\n",
    "num_tasks=25\n",
    "num_grad_steps=1\n",
    "meta_lr=1e-3\n",
    "step_lr=0.01\n",
    "foa = False\n",
    "config = 'maml'\n",
    "exp = True\n",
    "\n",
    "tst = MAML()\n",
    "\n",
    "start = time.time()\n",
    "hist_meta =tst.train(epochs, num_tasks, num_grad_steps, meta_lr, step_lr, foa, config, exp)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "#task = SineTask()\n",
    "#hist_tuned = tst.fine_tune(task)\n",
    "#tst.meta_model = SineModel()\n",
    "#tst.tuned_model = SineModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7918275 , 0.14464054],\n",
       "       [0.80967605, 0.14938773],\n",
       "       [0.81857073, 0.1522457 ],\n",
       "       [0.82973856, 0.15660688],\n",
       "       [0.8408081 , 0.16174957],\n",
       "       [0.85911393, 0.17167251],\n",
       "       [0.87739503, 0.18450785],\n",
       "       [0.89398545, 0.19813924],\n",
       "       [0.9070221 , 0.20832464],\n",
       "       [0.9322947 , 0.22832915],\n",
       "       [0.95315266, 0.24765098],\n",
       "       [0.9716045 , 0.26468158],\n",
       "       [0.9919823 , 0.28505173],\n",
       "       [1.0155514 , 0.3081302 ],\n",
       "       [1.0444933 , 0.33389965],\n",
       "       [1.07579   , 0.3661005 ],\n",
       "       [1.1023924 , 0.3917643 ],\n",
       "       [1.1335934 , 0.41771403],\n",
       "       [1.1706394 , 0.4530024 ],\n",
       "       [1.2083443 , 0.4924526 ],\n",
       "       [1.2424968 , 0.51789784],\n",
       "       [1.281462  , 0.55638194],\n",
       "       [1.3228526 , 0.5951663 ],\n",
       "       [1.356571  , 0.63443935],\n",
       "       [1.4005013 , 0.68353605],\n",
       "       [1.43925   , 0.7333797 ],\n",
       "       [1.4846143 , 0.790107  ],\n",
       "       [1.5351844 , 0.8406775 ],\n",
       "       [1.572209  , 0.8878728 ],\n",
       "       [1.6167601 , 0.9399789 ]], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, a2, a3 = tst.exp_results\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer1': [array([[0.0000000e+00, 1.7110252e-01, 1.1487588e-02, ..., 1.9846790e+00,\n",
      "        1.3524595e+00, 1.8962790e+00],\n",
      "       [1.7131019e-01, 2.4414062e-04, 1.5969111e-01, ..., 1.8135033e+00,\n",
      "        1.1812838e+00, 1.7251034e+00],\n",
      "       [1.1960399e-02, 1.5927646e-01, 0.0000000e+00, ..., 1.9728532e+00,\n",
      "        1.3406337e+00, 1.8844533e+00],\n",
      "       ...,\n",
      "       [1.9847580e+00, 1.8135208e+00, 1.9731386e+00, ..., 2.4414062e-04,\n",
      "        6.3216376e-01, 8.8344164e-02],\n",
      "       [1.3529217e+00, 1.1816847e+00, 1.3413023e+00, ..., 6.3189191e-01,\n",
      "        3.3430333e-04, 5.4349196e-01],\n",
      "       [1.8987775e+00, 1.7275404e+00, 1.8871580e+00, ..., 8.6036623e-02,\n",
      "        5.4618323e-01, 2.3670313e-03]], dtype=float32)], 'layer2': [array([[0.0000000e+00, 9.3392015e-01, 8.3665359e-01, ..., 8.4794998e-01,\n",
      "        9.0014833e-01, 8.7460899e-01],\n",
      "       [9.3392015e-01, 2.7295752e-04, 8.8525367e-01, ..., 8.5208249e-01,\n",
      "        9.9043739e-01, 7.7271730e-01],\n",
      "       [8.3665359e-01, 8.8525367e-01, 0.0000000e+00, ..., 9.7867775e-01,\n",
      "        8.8352323e-01, 7.5320899e-01],\n",
      "       ...,\n",
      "       [8.4794998e-01, 8.5208249e-01, 9.7867775e-01, ..., 2.7295752e-04,\n",
      "        7.5904506e-01, 7.6179349e-01],\n",
      "       [9.0014833e-01, 9.9043739e-01, 8.8352323e-01, ..., 7.5904512e-01,\n",
      "        0.0000000e+00, 7.2959095e-01],\n",
      "       [8.7460899e-01, 7.7271730e-01, 7.5320905e-01, ..., 7.6179349e-01,\n",
      "        7.2959089e-01, 0.0000000e+00]], dtype=float32)], 'layer3': [array([[0.05618285]], dtype=float32)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'layer1': [0.8494684, 0.6194956],\n",
       " 'layer2': [2.185856, 1.5487515],\n",
       " 'layer3': [0.05618285, 0.0]}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_m = MAML()\n",
    "test_m.train(800)\n",
    "test_m.exp_tasks = [SineTask() for m in range(1)]\n",
    "test_m.exp_weight_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over  10 tasks\n",
      "mean cca of relu1: 1.0000 std cca of relu1: 0.0000\n",
      "mean cca of relu2: 1.0000 std cca of relu2: 0.0000\n",
      "mean cca of layer3: 0.5357 std cca of layer3: 0.3376\n"
     ]
    }
   ],
   "source": [
    "tst.exp_similarity_CCA(num_tasks=10, shots=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight\n",
      "tensor([[-1.0501],\n",
      "        [ 0.9528],\n",
      "        [ 1.1293],\n",
      "        [-0.7479],\n",
      "        [-0.4293]], grad_fn=<SliceBackward>)\n",
      "%%%%\n",
      "tensor([[-1.0501],\n",
      "        [ 0.9282],\n",
      "        [ 1.0738],\n",
      "        [-0.7283],\n",
      "        [-0.4900]], grad_fn=<SliceBackward>)\n",
      "layer1.bias\n",
      "tensor([-3.9694, -0.8496, -2.3420,  0.2362,  1.4810], grad_fn=<SliceBackward>)\n",
      "%%%%\n",
      "tensor([-3.9694, -0.8664, -2.3732,  0.2250,  1.4810], grad_fn=<SliceBackward>)\n",
      "layer2.weight\n",
      "tensor([[ 2.0939e-02,  3.6316e-02, -8.2133e-02,  1.8672e-02, -3.7228e-03,\n",
      "         -3.8169e-02,  1.2538e-01, -1.9163e-02, -4.7080e-02,  3.5004e-03,\n",
      "          9.9766e-02, -1.2179e-01,  1.0101e-01, -7.6037e-02,  1.5119e-01,\n",
      "         -3.5790e-02, -7.8296e-03, -1.2968e-01, -2.2738e-02, -1.0661e-01,\n",
      "         -1.5090e-01,  1.4161e-01, -1.2062e-01, -5.2631e-02, -1.4661e-01,\n",
      "         -5.7123e-02,  1.5666e-01,  4.5315e-02, -1.3469e-01, -1.2833e-01,\n",
      "         -4.6770e-02,  6.5010e-02,  6.2364e-02,  1.0040e-01, -1.4817e-01,\n",
      "          1.1422e-01, -1.5231e-01, -2.1088e-02, -3.8727e-02,  1.4285e-01],\n",
      "        [-1.1318e-01, -4.3309e+00,  9.1108e-02, -8.0332e-02,  2.0559e-01,\n",
      "          1.4485e-01, -3.8712e-02, -2.8746e+00, -1.0101e-01, -3.7406e+00,\n",
      "         -2.1534e+00, -6.3296e-01,  1.0103e-01, -2.4808e-01, -2.5160e-02,\n",
      "          1.1914e+00, -1.5712e+00,  2.4643e-01,  2.2093e-01, -2.1056e-01,\n",
      "         -8.6087e-01, -3.5202e+00, -6.2294e-01, -1.7545e-02, -8.4414e-01,\n",
      "          5.2319e-02, -7.4858e-01, -1.2940e-01,  2.5363e-03,  1.3602e-03,\n",
      "          1.0093e+00, -9.9658e-02,  1.4109e-01,  9.5548e-02,  6.4105e-02,\n",
      "         -5.6472e-01, -6.7430e-02,  3.1069e-01, -8.8063e-01, -1.6871e+00],\n",
      "        [ 5.2333e-02, -1.2945e-01,  1.4183e-01, -4.3456e-02, -1.2496e-01,\n",
      "          4.6714e-02,  4.7977e-02,  8.8564e-02, -2.5257e-02, -1.2095e-01,\n",
      "          6.9369e-02,  5.5860e-02,  9.9506e-02,  4.9581e-02,  1.2381e-01,\n",
      "          4.5853e-03, -6.7327e-02, -2.0154e-01, -1.8299e-01,  8.8793e-02,\n",
      "         -6.7667e-03,  2.0584e-02, -6.4004e-02, -3.9351e-01,  7.2776e-02,\n",
      "         -5.6809e-01, -1.6299e-02,  1.0276e-01, -1.1557e-01, -8.5687e-02,\n",
      "         -2.0858e-01, -1.1286e-01,  1.1155e-01, -1.1243e-01, -2.6535e-01,\n",
      "         -2.2422e-04, -5.0691e-02,  5.2953e-02, -1.7252e-01, -7.8986e-02],\n",
      "        [-1.5810e-01,  1.3276e-02, -1.0844e-01,  1.0007e-01, -1.2354e-01,\n",
      "         -1.9949e-01,  9.9148e-02,  1.8548e-01,  4.5069e-02, -1.3844e-01,\n",
      "         -7.2550e-02, -4.9059e-02,  1.0589e-01,  1.5247e-01, -1.0229e-01,\n",
      "         -1.5155e-01,  7.1058e-02,  1.0654e-02, -3.8972e-01, -1.0045e-01,\n",
      "          8.5071e-02,  1.3491e-01, -1.4763e-01, -1.1961e+00,  2.0880e-02,\n",
      "         -1.1473e+00, -4.0427e-03, -1.1875e-01, -4.4870e-02, -7.5643e-02,\n",
      "          9.6559e-03, -1.5913e-01, -1.5626e-01, -2.4826e-02, -2.2760e-01,\n",
      "         -1.2277e-02, -5.6078e-02,  1.1101e-01,  1.6133e-01,  9.4644e-02],\n",
      "        [-7.4971e-01,  2.6118e-01,  5.2548e-02, -4.3258e-01,  3.9331e-01,\n",
      "         -2.1037e-01, -3.4696e-01,  1.2720e-01,  8.5019e-03, -6.1807e-01,\n",
      "          4.2908e-01,  2.3073e-02, -1.3116e-01, -6.5152e-01, -3.5504e-01,\n",
      "         -9.2711e-01, -3.5198e-01, -8.2282e-02,  4.1828e-01, -7.8605e-01,\n",
      "         -2.4466e-01, -3.8259e-01, -5.9604e-03,  1.0098e-01,  8.7191e-02,\n",
      "          1.5333e-01, -2.9413e-02, -6.8507e-01, -4.1683e-01, -1.1425e+00,\n",
      "         -9.7448e-01, -1.7699e+00, -7.5387e-01, -1.1685e+00,  1.1461e-02,\n",
      "         -5.3792e-02, -8.6067e-02, -3.8178e-01,  4.5000e-01, -3.5826e-01]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "%%%%\n",
      "tensor([[ 2.0939e-02,  3.6316e-02, -8.2133e-02,  1.8672e-02, -3.7228e-03,\n",
      "         -3.8169e-02,  1.2538e-01, -1.9163e-02, -4.7080e-02,  3.5004e-03,\n",
      "          9.9766e-02, -1.2179e-01,  1.0101e-01, -7.6037e-02,  1.5119e-01,\n",
      "         -3.5790e-02, -7.8296e-03, -1.2968e-01, -2.2738e-02, -1.0661e-01,\n",
      "         -1.5090e-01,  1.4161e-01, -1.2062e-01, -5.2631e-02, -1.4661e-01,\n",
      "         -5.7123e-02,  1.5666e-01,  4.5315e-02, -1.3469e-01, -1.2833e-01,\n",
      "         -4.6770e-02,  6.5010e-02,  6.2364e-02,  1.0040e-01, -1.4817e-01,\n",
      "          1.1422e-01, -1.5231e-01, -2.1088e-02, -3.8727e-02,  1.4285e-01],\n",
      "        [-1.1318e-01, -4.3309e+00,  9.1108e-02, -8.0332e-02,  1.9300e-01,\n",
      "          1.3514e-01, -3.8712e-02, -2.8746e+00, -1.0101e-01, -3.7406e+00,\n",
      "         -2.1534e+00, -6.3296e-01,  1.0103e-01, -2.4808e-01, -2.5160e-02,\n",
      "          1.1889e+00, -1.5712e+00,  2.2865e-01,  2.0553e-01, -2.1056e-01,\n",
      "         -8.6087e-01, -3.5202e+00, -6.2294e-01, -3.3465e-02, -8.5265e-01,\n",
      "          4.0370e-02, -7.4858e-01, -1.2947e-01,  2.5363e-03,  1.3602e-03,\n",
      "          1.0073e+00, -9.9658e-02,  1.4109e-01,  9.1638e-02,  1.4428e-02,\n",
      "         -5.6472e-01, -6.7430e-02,  3.1069e-01, -8.8344e-01, -1.6871e+00],\n",
      "        [ 5.2333e-02, -1.2945e-01,  1.4183e-01, -4.3456e-02, -1.2496e-01,\n",
      "          4.6714e-02,  4.7977e-02,  8.8564e-02, -2.5257e-02, -1.2095e-01,\n",
      "          6.9369e-02,  5.5860e-02,  9.9506e-02,  4.9581e-02,  1.2381e-01,\n",
      "          4.5853e-03, -6.7327e-02, -2.0154e-01, -1.8299e-01,  8.8793e-02,\n",
      "         -6.7667e-03,  2.0584e-02, -6.4004e-02, -3.9351e-01,  7.2776e-02,\n",
      "         -5.6809e-01, -1.6299e-02,  1.0276e-01, -1.1557e-01, -8.5687e-02,\n",
      "         -2.0858e-01, -1.1286e-01,  1.1155e-01, -1.1243e-01, -2.6535e-01,\n",
      "         -2.2422e-04, -5.0691e-02,  5.2953e-02, -1.7252e-01, -7.8986e-02],\n",
      "        [-1.5810e-01,  1.3276e-02, -1.0844e-01,  1.0007e-01, -1.2354e-01,\n",
      "         -1.9949e-01,  9.9148e-02,  1.8548e-01,  4.5069e-02, -1.3844e-01,\n",
      "         -7.2550e-02, -4.9059e-02,  1.0589e-01,  1.5247e-01, -1.0229e-01,\n",
      "         -1.5155e-01,  7.1058e-02,  1.0654e-02, -3.8972e-01, -1.0045e-01,\n",
      "          8.5071e-02,  1.3491e-01, -1.4763e-01, -1.1961e+00,  2.0880e-02,\n",
      "         -1.1473e+00, -4.0427e-03, -1.1875e-01, -4.4870e-02, -7.5643e-02,\n",
      "          9.6559e-03, -1.5913e-01, -1.5626e-01, -2.4826e-02, -2.2760e-01,\n",
      "         -1.2277e-02, -5.6078e-02,  1.1101e-01,  1.6133e-01,  9.4644e-02],\n",
      "        [-7.4971e-01,  2.4283e-01,  4.7395e-02, -4.3258e-01,  3.7424e-01,\n",
      "         -2.4456e-01, -3.4696e-01,  1.1160e-01,  8.5019e-03, -6.1807e-01,\n",
      "          4.0914e-01,  2.3073e-02, -1.3116e-01, -6.5152e-01, -3.5504e-01,\n",
      "         -9.4376e-01, -3.5198e-01, -9.9242e-02,  3.9459e-01, -7.8605e-01,\n",
      "         -2.4466e-01, -3.8259e-01, -5.9604e-03,  7.6386e-02,  4.6194e-02,\n",
      "          1.4151e-01, -3.9022e-02, -6.8507e-01, -4.1683e-01, -1.1425e+00,\n",
      "         -9.8931e-01, -1.7699e+00, -7.5387e-01, -1.1712e+00, -1.0191e-01,\n",
      "         -5.3792e-02, -8.6067e-02, -3.9651e-01,  4.2110e-01, -3.5826e-01]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "layer2.bias\n",
      "tensor([-0.0833, -0.2021, -0.3024, -0.1931,  0.0477], grad_fn=<SliceBackward>)\n",
      "%%%%\n",
      "tensor([-0.0833, -0.2123, -0.3024, -0.1931,  0.0234], grad_fn=<SliceBackward>)\n",
      "layer3.weight\n",
      "tensor([[-1.6982e-01,  1.4827e+00, -1.0028e-02, -4.6582e-03, -1.2029e+00,\n",
      "          1.7649e-02, -6.3873e-02, -2.1453e-03, -5.3472e-02, -4.9274e-02,\n",
      "          7.9761e-03, -1.8670e-02,  3.5673e-02,  7.1206e-01,  1.4095e-02,\n",
      "         -5.6379e-03,  2.1600e-01,  3.4264e-02, -1.2125e-01,  7.3241e-02,\n",
      "         -9.7137e-02, -6.2781e-04,  2.3567e-01, -2.0918e-01,  2.3816e-02,\n",
      "          8.4021e-03,  2.3681e-01, -2.1012e-01,  2.7433e-02, -9.1331e-02,\n",
      "          5.4799e-02, -9.6949e-02, -6.9079e-02, -1.0244e-01,  9.9829e-02,\n",
      "         -2.2679e-01, -2.2699e-01, -5.2481e-02, -1.5083e-01,  1.9714e-02]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "%%%%\n",
      "tensor([[-1.6982e-01,  1.4773e+00, -1.0028e-02, -4.6582e-03, -1.1955e+00,\n",
      "          1.7649e-02, -6.3873e-02, -2.1453e-03, -5.3472e-02, -4.9274e-02,\n",
      "          7.9761e-03, -1.8670e-02,  3.5673e-02,  6.0862e-01,  1.4095e-02,\n",
      "         -5.6379e-03,  2.9077e-01,  3.4264e-02, -1.2125e-01,  7.3241e-02,\n",
      "         -9.7137e-02, -6.2781e-04,  2.3567e-01, -2.0918e-01,  2.3816e-02,\n",
      "          8.4021e-03,  6.3538e-01, -2.1012e-01,  2.7433e-02, -9.1331e-02,\n",
      "          5.4799e-02, -9.6949e-02, -6.9079e-02, -1.0244e-01,  9.9829e-02,\n",
      "         -2.2679e-01, -2.2699e-01, -5.2481e-02, -1.5083e-01,  1.9714e-02]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "layer3.bias\n",
      "tensor([-3.1940], grad_fn=<SliceBackward>)\n",
      "%%%%\n",
      "tensor([-3.1862], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5502)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 10\n",
    "data_a, data_b = tst.CCA_test(num_samples)\n",
    "\n",
    "# since this should be relu activation. it should not contain negatives\n",
    "#data_a\n",
    "\n",
    "data_ta = torch.stack(data_a)\n",
    "data_tb = torch.stack(data_b)\n",
    "a, b, diag = svcca(data_ta, data_tb)\n",
    "\n",
    "#print(data_ta[:,1])\n",
    "#print(data_tb[:,1])\n",
    "\n",
    "for ((name, par1), par2) in zip(tst.meta_model.named_parameters(),tst.tuned_model.parameters()):\n",
    "    print(name)\n",
    "    print(par1[:5])\n",
    "    print('%%%%')\n",
    "    print(par2[:5])\n",
    "\n",
    "\n",
    "\n",
    "torch.mean(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'layer1.bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-9ea4406151f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(name.split('.')[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layer1.bias'"
     ]
    }
   ],
   "source": [
    "dic = {'layer1.weight': []}\n",
    "m = SineModel()\n",
    "for name, parameter in m.named_parameters():\n",
    "    #print(name.split('.')[1])\n",
    "    dic[name].append(4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb619585c50>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU9b3/8ddnZpJMJnsyQwJkYwkoiggEBDG5LtW6a9VrbavV1pZiF3u73tr7u+3tetvbaluXaqm2V1tv3aqtVetWrCACGkAQZAl7EiAbJGRfv78/zmQlkAmZyZmZfJ6Px3nkzJxvZj4ZyHu++c73fI8YY1BKKRX5HHYXoJRSKjg00JVSKkpooCulVJTQQFdKqSihga6UUlHCZdcTe71ek5+fb9fTK6VURFq/fn2NMcY31DHbAj0/P5+SkhK7nl4ppSKSiOw/0bGAAl1E9gENQBfQaYwpHHT8fOCvwF7/Xc8aY75/KsUqpZQ6NSPpoV9gjKk5yfFVxpgrR1uQUkqpU6MfiiqlVJQINNAN8KqIrBeRpSdos1hENonI30XkjKEaiMhSESkRkZLq6upTKlgppdTQAh1yOc8YUyEiE4DXRGS7MWZlv+MbgDxjTKOIXA78BSgY/CDGmOXAcoDCwkJdREYppYIooB66MabC/7UKeA5YOOj4MWNMo3//JSBGRLxBrlUppdRJDBvoIpIgIkk9+8AlwJZBbbJERPz7C/2PWxv8cpVSSp1IID30TOAtEdkEvAO8aIx5WUSWicgyf5sbgC3+NvcCN5kQrcu7s7KBH7zwAa0dXaF4eKWUiljDjqEbY/YAc4a4/6F++/cD9we3tKGVH23mkbf2cuFpE1gyXUd1lFKqR8RNWzxnSgYxTmFlqc6SUUqp/iIu0BPiXBTmpbNy58nOcVJKqfEn4gIdoGiGl22HjlHV0Gp3KUopFTYiMtCLC6yFxlbv0l66Ukr1iMhAnzUxmfSEWFbpsItSSvWKyEB3OITzpntZWVpDiGZHKqVUxInIQAcoKvBS09jG9sMNdpeilFJhIYID3RpHX7lTpy8qpRREYqBXrIdHryYrro2ZmUmsKtVxdKWUgkgMdGNg3yp45T8oKvDyzr4jtLTrMgBKKRV5gZ5dCEu+DBv/wDUJW2nv7OadfUfsrkoppWwXeYEOcP5d4DudM9f/J15XM6t0HF0ppSI00F1x8JEHkaYqfpH8hI6jK6UUkRroAJPmQvHXKWp+ndzqNzhcr8sAKKXGt8gNdICir9OScQY/jnmEdVtL7a5GKaVsFdmB7orFfcNDpEoT2Wu+Y3c1Sillq8gOdEAmnsVrvluZ37CC7i1/sbscpZSyTcQHOkD7oi+xuXsK3S98BRp1xotSanyKikA/d8ZEvtZxB9LWAC9+1Tr5SCmlxpmoCPQJSW5cWbN4MvEW2PY8bPmz3SUppdSYi4pABygu8PK9IxfRNakQXvwaNBy2uySllBpTURPoRQU+2rqEd8/+IXS2wt/+TYdelFLjSkCBLiL7ROR9EXlPREqGOC4icq+I7BKRzSIyL/ilnlxhfhruGAcvH06Gi74DO/8Om54Y6zKUUso2I+mhX2CMOdsYUzjEscuAAv+2FHgwGMWNhDvGyTlTMlhVWg3nLIPcxfD3f4f6irEuRSmlbBGsIZdrgMeMZS2QKiITg/TYASsq8LK7uomKY+1wzQPQ3QF/u1OHXpRS40KggW6AV0VkvYgsHeL4ZKCs3+1y/30DiMhSESkRkZLq6uDPFy+eYV3F6K3SasiYBh/6Hux6HTY8FvTnUkqpcBNooJ9njJmHNbTyBREpPpUnM8YsN8YUGmMKfT7fqTzESRVMSCQzOY6VO/2rLy74DOQXwSv/AXUHgv58SikVTgIKdGNMhf9rFfAcsHBQkwogp9/tbP99Y0pEKCrw8dauGrq6DTgc1tALBv76RejuHuuSlFJqzAwb6CKSICJJPfvAJcCWQc2eBz7pn+2yCKg3xhwKerUBKJ7ho76lg/cr6q070vLgkh/C3jdh/e/sKEkppcZEID30TOAtEdkEvAO8aIx5WUSWicgyf5uXgD3ALuC3wOdDUm0AzpvuRYSBVzGafxtMvQBe/Q4c2WtXaUopFVJibJoBUlhYaEpKjpvSHhRX3fcW8TFOnlq2uO/O+nL49WLImg23vmANxyilVIQRkfUnmD4ePWeK9ldU4GXDgaM0tHb03ZmSDZf+N+xfDe/8xr7ilFIqRKI00H10dhvW7jky8MDZn4CCS+D170HNLnuKU0qpEInKQJ+Xl4on1snKnYPmuovAVfeCKxb+cgd0d9lToFJKhUBUBnqcy8miqf5lAAZLngiX/QzK34E1D4x9cUopFSJRGehgLae7r7aZA7XNxx8860Y47UpY8UOo2j72xSmlVAhEbaAX+ZcBWLVriF66CFz5C4hNsIZeujrHuDqllAq+qA30qd4EJqfGs6pnGYDBEifAFXfDwQ2w+pdjW5xSSoVA1Aa6tQyAl9W7a+jsOsEp/2deB7OuhX/+BCq3jm2BSikVZFEb6GBNX2xo7WRTed2JG11xD8SnwnPLoKvjxO2UUirMRXWgL5megQh9qy8OJSHDGk8/vBlW3T12xSmlVJBFdaCnemI5Kzt16OmL/Z1+Fcy+EVb+DA5tGpvilFIqyKI60AH+pcDLe2V11LcMM5xy2U/B44Xn7oDOtrEpTimlgijqA71oho9uA2t2n2TYBcCTDlf9Cqq2wpv/MzbFKaVUEEV9oJ+dk0pinIuVpcMEOsDMS631Xt76BVSsD31xSikVRFEf6DFOB4unZbByZzUBLRX84R9DYqY19NLRGvoClVIqSKI+0MFaBqD8aAv7hloGYLD4VLjmPqjZAW/8KPTFKaVUkIyLQC8q8C8DMNxslx7TPwTzboW374MD60JYmVJKBc+4CPR8bwK56Z6Tz0cf7MM/gpQca62X9gB69kopZbNxEehgXcVoze4aOk60DMBgcUlwzf1wZDes+EFoi1NKqSAYR4Huo6m9i40HTrIMwGBT/wUWfBbWPgj7VoeuOKWUCoJxE+iLp2XgdEjg4+g9PvRfkJYHf/08tDWGojSllAqKcRPoKfExnJ2Tevxl6YYTlwjXPghH98Pr3w1NcUopFQQBB7qIOEVko4i8MMSx20SkWkTe82+fCW6ZwVFU4GVzRT1Hm9pH9o1558KiO+Ddh2HPP0NSm1JKjdZIeuhfBrad5PiTxpiz/dvDo6wrJIoKfBgDq4dbBmAoF/4nZEyHv34RWo8FvzillBqlgAJdRLKBK4CwDOpAzclOIdntOvFVjE4m1mMNvRyrgFf/X/CLU0qpUQq0h/5L4JvAyeb8XS8im0XkGRHJGaqBiCwVkRIRKamuHuFYdhC4nA6WTPeyqjTAZQAGy1kI534JNjwKpa8Hv0CllBqFYQNdRK4EqowxJ1ut6m9AvjHmLOA14NGhGhljlhtjCo0xhT6f75QKHq2iAh8H61vZXd10ag9w/rfBOxOe/xK0jGAKpFJKhVggPfQlwNUisg94ArhQRP7Yv4ExptYY07OI+MPA/KBWGURFBV5gBMsADBbjho88CI2V8NI3oDvAE5WUUirEhg10Y8xdxphsY0w+cBOwwhhzc/82IjKx382rOfmHp7bKSfcwxZsw8umL/U2eD8XfgPefgj9cCw2Hg1egUkqdolOehy4i3xeRq/037xSRrSKyCbgTuC0YxYVKUYGXtXuO0NbZdeoPcv63rAtilL0DDy6B0teCV6BSSp2CEQW6Meafxpgr/fvfMcY879+/yxhzhjFmjjHmAmPM9lAUGyxFBT5aOrpYv//oqT+ICMy/DZb+01o//fEb4OVv6+XrlFK2GTdniva3eFoGLoewKpCrGA1nwmnw2X/Ags/A2gfgkYuhdvfoH1cppUZoXAZ6YpyLeXlpp/7B6GAx8XDF3fDRx60lAn5TDJueCM5jK6VUgMZloIN1FaMtFceobQziEMnpV8IdqyHrLHjuc/Ds56CtIXiPr5RSJzFuA73nKkZv7QrCsEt/Kdlw69/gX75lzYL5TTEc3Bjc51BKqSGM20A/c3IKqZ6YkV3FKFBOF1xwlxXsHa3w8MWw5gE4lbNTlVIqQOM20J0OGd0yAIHIP88agim4BF75NvzfjdA49kseKKXGh3Eb6GCNo1c1tLGzMoQXrvCkw02Pw+U/hz1vwkNLdAlepVRIjOtA7xlHD9pslxMRgYWftaY3ulPgsWvh9e9BV0don1cpNa6M60CflBrP9AmJrAzGfPRAZM22TkSaezO8dQ/8/jJrmqNSSgXBuA50sJYBWLenltaOUSwDMBKxCXDN/XDD76B6BzxUBFufG5vnVkpFtXEf6MUFPto6u3l335GxfeIzr4dlq8BbAE/fZi3H2948tjUopaLKuA/0c6amE+MM0jIAI5WWD59+Gc77Cmz4Ayw/Hyq3jn0dSqmoMO4D3RProjAvfXTL6Y6GMwY+9F9wy3PQWgfLL4B3fqtz1pVSIzbuAx2geIaP7YcbqDrWal8R0y6AZathSjG89HV48mZoHuNhIKVURNNAp+8qRkFfBmCkEn3w8afgkh/BzlfgofNg/9v21qSUihga6MCsiclkJMTaM44+mMMB534Rbn8VXHHwv1fAP38C3WM0C0cpFbE00AGHQzivwFoGoLs7TMauJ8+Dz62E2f8K//xvePQqqK+wuyqlVBjTQPcrKvBR09jOtsPH7C6lT1wSXLccPvIbOPietWzA9hftrkopFaY00P16xtHDYthlsDk3WXPWU3PhiY/DS9+wVnFUSql+NND9MpPdzMxMCv26LqcqYxrc/hos+gK8sxwevsg601Qppfw00PspnuHl3b1HaWkP0w8gXXFw6Y/h409DwyHrRKQNj+mcdaUUMIJAFxGniGwUkReGOBYnIk+KyC4RWSci+cEscqwUFfho7+pm3d5au0s5uRmXWHPWswutJQMe/1cofR26u+2uTCllo5H00L8MbDvBsduBo8aY6cAvgJ+OtjA7LJySTqzLEZqrGAVb8kS45S9w8fetS9w9fj3cOwdW/hwaKu2uTillg4ACXUSygSuAh0/Q5BrgUf/+M8BFIiKjL29suWOcnDMlPXzH0QdzOGHJl+GrH1irN6blw4ofwC9mWWea7vqH9tqVGkcC7aH/EvgmcKJ0mAyUARhjOoF6IGNwIxFZKiIlIlJSXR2eoVlU4KW0qpFD9S12lxI4V5y1euOtf4MvrodFd1hnmP7xOrhvLqy6Bxqr7K5SKRViwwa6iFwJVBlj1o/2yYwxy40xhcaYQp/PN9qHC4m+qxhFwLDLULzT4ZIfwle3wfWPQHI2/ON7cM/p8NQnYfcb2mtXKkoF0kNfAlwtIvuAJ4ALReSPg9pUADkAIuICUoAw/2RxaKdlJeFLiovcQO/hioPZN8CnXoQvlsA5y2DvSvjDtXDfPHjrl3rBaqWizLCBboy5yxiTbYzJB24CVhhjbh7U7HngVv/+Df42ETmXTkQoKvDyVjgtAzBa3gL48I/gq9vhut9C0kR4/btWr/3p26yLV0fmP5dSqp9TnocuIt8Xkav9Nx8BMkRkF/BV4FvBKM4uxQU+jjZ3sPVgGC0DEAwxbjjrRvj03+EL71gXrt79Bjx2Ndw3H1bfC00R+YeVUgoQuzrShYWFpqSkxJbnHk51QxsLfvQ63/jwTL5wwXS7ywmtjhb44K9Q8nsoWwvOWDj9Kpj/Kcg/DyJvspJSUU1E1htjCoc6pmeKDsGXFMesicn2XcVoLMXEW2vF3P4KfH4tFH4adr0Oj14J9y+At+/XC20oFSE00E+gaIaXDQeO0tjWaXcpY2fC6XDZT62x9msfhPg0ePU/4O6Z8OfPwL7VOtauVBjTQD+B4gIfHV2GdXvG4ZhyrAfO/jh85jW4422YfxvsfBX+93J44BxY82vttSsVhjTQT2B+XhruGEfkT18crcwz4PKfwde2wzUPWGu0v3IX3H0aPLsU9q/RXrtSYcJldwHhyh3jZNHUDFZGyjIAoRbrgbk3W9vh92H9/8Lmp2Dzk+A7zfoQ9YxrISnL7kqVGre0h34SRQU+9lQ3UX602e5SwkvWbLjibqvXfvV9EOOBl//dGmv/1Rx4bpkV+FXb9axUpcaQ9tBPorjfVYw+tjDX5mrCUGwCzPuktR3eAnvfhANrrFkym/5ktYlPg5xFkHsO5C6GSXOts1iVUkGngX4S0yckkpXsZlVptQb6cLLOtLbFX7DG1I/sgQNrrYA/sBZ2/t1q54yzLoCd4w/4nIXgSbe3dqWihAb6SfQsA/DK1sN0dRucDj3JJiAi1iXzMqbB3E9Y9zXVQNk6f8CvgzUPwOpfWsd8p0HuIn/An2MtA6wnNCk1Yhrowyia4ePp9eVsLq9jbm6a3eVErgQvnHaFtYF1hmrFBivgy9bBluescXeAxKy+gM89BzJng1P/qyo1HP0tGcZ5072IWOPoGuhBFBMP+UusDawPT6u3+Ydp/NsHf/G3TYCcBf6x+EWQvQDiEu2rXakwpYE+jPSEWGZPTmFVaTV3XlRgdznRy+Gw5rxnngELbrfuq6+w1pfpGYtf+T9gukGc1nh97mIr4HMWWZfkU2qc00APQFGBl4fe3MOx1g6S3TF2lzN+pEyGlOutqzEBtB6D8netgC9bCxseg3UPWcdS8/oN0ZwJGdP1w1Y17migB6CowMcDb+xmze5aPnyGnjhjG3cyTL/I2gC6OuDw5r4hmt0rYPMTfe09GZBRYF3FKaPAWhc+o8D60NUVa8uPoFQoaaAHYF5uGp5YJ6tKqzXQw4kzBibPt7ae6ZJH90H1DqgthZpSqN1lrUPT1O8iW+K0Qt1bYPXke4LeWwAJPp1hoyKWBnoAYl0OFk/N0HVdwp0IpE+xNi4deKy1Hmp29Qv6Uuv2nn9CZ2tfu7gUf49++sDefcY064NcpcKYBnqAigq8/GN7Fftrm8jLSLC7HDVS7hTInm9t/XV3w7Hyvt58T9jvW22tU9NLICVn0PCNv3efPFl79SosaKAHqHiGD7CmL2qgRxGHA1Jzra1nbL5HexPU7u7rzff07sseh/bGvnYxHv+JVP3G6TOmWR/UetI17NWY0UAP0BRvApNT41m5s5qbF+XZXY4aC7EJMPEsa+vPGGg4PHCcvqYUDm6w5s6bfguSueIhJfsEW47Vu49xj+3PpaKWBnqARITiGV5e2HSIjq5uYpy6UOW4JWLNe0+eCFOKBx7rbLPWsandDfXlUF/m/1oOpa9B4+HjHy/Bd3zI9+ynZFvHHfr/TQ1PA30Eigp8/OmdMjaV1VGYr3Oc1RBccdal/CacPvTxzjY4dtAK+GMVAwO/phR2rYCOpoHf44w9PuRTBt2O1WFApYE+IudOy8AhsLK0RgNdnRpXXL+ZOEMwBlrr+kJ+8LZ3JTQcHDisA9YyxQMCP7vvTSAx09piPaH/+ZSthg10EXEDK4E4f/tnjDHfHdTmNuBnQIX/rvuNMQ8Ht1T7pXpiOSs7lVWl1Xz14hl2l6OikYgVzvFp1oVEhtLVCQ2H+gV9v15+3QHYv9qapjlYbBIk+cM9cUJf0PdsPcc8GeBwhvbnVCERSA+9DbjQGNMoIjHAWyLyd2PM2kHtnjTGfDH4JYaX4gIv97+xi/rmDlI8ugyAsoHTBak51nYircf8QzoV0Fjp36qsMfzGKusygo3/gLZjx3+vOKxx+wGBP8G6vODgNwJdJC2sDBvoxhgD9MzRivFv4/aqwMUzfNy7Yhdv767hstm6IJQKU+5kazvRWH6P9iZ/0Ff1C/5+bwANh6FyKzRVQXfn8d8fk9AX8sf1/vu9AST4dAnkMRDQKywiTmA9MB14wBizbohm14tIMbAT+IoxpmyIx1kKLAXIzY3MKwDNyUklKc7FytJqDXQV+WITTj6m36O7G1qO+nv4lf3eAPyh31hpXUN2z5vWZwBDiU+31sVP8PX76rOGeHr2e465U3VmzykIKNCNMV3A2SKSCjwnImcaY7b0a/I34E/GmDYR+RzwKHDhEI+zHFgOUFhYGJG9/Bing8XTMli5swZjDKInjajxwOGAhAxryzzj5G07Wq0efU/oN/iHeZprrCtXNdVY4d+0ClqODP0Y4hwY/p4h3ggSvH23YxP1BC5GOMvFGFMnIm9gLZSxpd/9tf2aPQz8T3DKC09FM3y8+kEle2uamOrTMUSlBohx9519O5yuTmiu9Yd9tT/wq/vt+28fXW/ttzcM/Tgu9xC9/cFvAv43Bk9G1M74CWSWiw/o8Id5PHAx8NNBbSYaYw75b14NbAt6pWGkuMALWMsAaKArNQpOlzX2npQZWPuO1hOEfzU01fbtV22zvna1Df04MR4r2D3pfSHv8f8F0rPf/35PekTM/Amkhz4ReNQ/ju4AnjLGvCAi3wdKjDHPA3eKyNVAJ3AEuC1UBYeDvIwEctM9rCqt5tZz8+0uR6nxI8bdN89+OMZYa+70hH9jlf+vgSG22l3QfOTEfwEgEJ86KOj9nwkMfgPoeVOwYRgokFkum4G5Q9z/nX77dwF3Bbe08FY8w8vTJeWs2V3L4mkZdpejlBpMBOKSrC19amDf09k2MOibaqyg772vxvpatx8q1lv73R1DP5Yz9sThn7cEphQF72f103lEp+hLFxawbs8Rbv3dO9x94xyumjPJ7pKUUqPlioPkSdYWCGOgrcEf9Ef6vQkM8VfAoU3WsdY6KPq6Bno4yUx288yyc/nsYyV86U8bqWpo4/bzhpn6pZSKLiJ9c/4D/Sugq/PEvfpR0omeo5DiieGx2xdy6RlZ/OCFD/jxS9vo7o7I2ZhKqbHidIXs6lca6KPkjnHywCfm8cnFeSxfuYevPPUe7Z3dw3+jUkoFmQ65BIHTIXzv6jOYmBLPT1/eTk1jGw/dPJ8kt671opQaO9pDDxIR4Y7zp3HPjXNYt+cIN/5mLZXHWof/RqWUChIN9CC7bl42v7ttAftrm7ju12+zq6px+G9SSqkg0EAPgeIZPp5cupi2zi5ueOht1u8/wXoVSikVRBroITI7O4Vn71hCmieWj/92Ha9uHeJakkopFUQa6CGUm+HhmWWLOW1iMsv+uJ7H1+23uySlVBTTQA+xjMQ4/vTZczh/5gT+47kt3P3qDqxrhiilVHBpoI8BT6yL5bfM56OFOdy3YhfffGYzHV06V10pFVw6D32MuJwOfnL9bLJS3PzqH6VUN7bx60/MwxOr/wRKqeDQHvoYEhG+cvEM/vu62azcWc3Hlq+lpvEE6zUrpdQIaaDb4GMLc1l+SyE7Khu44cG32V/bZHdJSqkooIFukw/NyuT/PruI+pYOrvv122wuP8GFdZVSKkAa6Daal5vGM3ecS3ysk5uWr+WNHVV2l6SUimAa6Dab5kvk2TvOJT8jgc88WsLTJWV2l6SUilAa6GFgQrKbJz+3iMVTM/jGM5u5f0WpzlVXSo2YBnqYSHLH8LvbFnDt2ZP4+as7+c5ft9KlF8tQSo2AToIOI7EuB/fceDaZKW5+8+YeKo+1cu/H5uKOcdpdmlIqAmgPPcw4HMJdl53Od6+axWvbKvnEw+uoa263uyylVAQYNtBFxC0i74jIJhHZKiLfG6JNnIg8KSK7RGSdiOSHotjx5FNLpnD/x+bxfnk91z/4NuVHm+0uSSkV5gLpobcBFxpj5gBnA5eKyKJBbW4HjhpjpgO/AH4a3DLHpyvOmshjty+kqqGN6379Nh8cPGZ3SUqpMDZsoBtLz2V3Yvzb4E/rrgEe9e8/A1wkIhK0KsexRVMzeGbZuThE+Ohv1vD2rhq7S1JKhamAxtBFxCki7wFVwGvGmHWDmkwGygCMMZ1APZAxxOMsFZESESmprq4eXeXjyMysJJ79/LlMTHVz6+/f4flNB+0uSSkVhgIKdGNMlzHmbCAbWCgiZ57KkxljlhtjCo0xhT6f71QeYtyalBrP0587l7m5adz5p408vGqP3SUppcLMiGa5GGPqgDeASwcdqgByAETEBaQAtcEoUPVJ8cTw2KcXcvnsLH744jZ+8MIHdOtcdaWUXyCzXHwikurfjwcuBrYPavY8cKt//wZghdFTHUPCHePkvo/N47Zz83nkrb3c+cRG2jq77C5LKRUGAjmxaCLwqIg4sd4AnjLGvCAi3wdKjDHPA48AfxCRXcAR4KaQVaxwOoTvXjWLrBQ3P/n7dnZVNXLL4jyunjOJJHeM3eUppWwidnWkCwsLTUlJiS3PHU1eev8Q9/6jlO2HG4iPcXL57InctDCHwrw0dKKRUtFHRNYbYwqHOqan/ke4y2dP5LIzs9hcXs8T75bxt00H+fOGcqb6EvhoYQ7XzcvGlxRnd5lKqTGgPfQo09zeyYubD/Hku2WU7D+KyyFcdPoEblqQS/EMH06H9tqVimQn66FroEexXVWNPFVSxp/Xl1Pb1M7EFDc3zM/mxsIcctI9dpenlDoFGujjXHtnNyu2V/LEu2W8ubMaY+C86V5uXJDDJbMydTVHpSKIBrrqdbCuhWfWl/Pku2VU1LWQ6onhI3Mn89EFOZyWlWx3eUqpYWigq+N0dxtW767hyXfLeHVrJe1d3czJSeWmBTlcedZEnf6oVJjSQFcndbSpnec2VvDku2XsqLSmP155ljX9cV6uTn9UKpxooKuAGGN4r6yOp0rKeP69gzS1dzHNl8BNC3L5yLzJeBN1+qNSdtNAVyPW1Oaf/lhSxvr9R4lxCh86PZOPLsihqECnPyplFw10NSqllQ08+W4Zz26s4EhTO5NS3NxQmMO/zs/W6Y9KjTENdBUU7Z3dvL7Nmv64qtRaz/686V4+uiCHi2dlEufS6Y9KhZoGugq6iroWni4p4+mScirqWkjzxHDBaROYl5vGvNw0ZmYl6bCMUiGgga5CpqvbsHpXDU+VlLFmdy21Te0AJMQ6mZOTagV8Xipzc9JIS4i1uVqlIp8uzqVCxukQimf4KJ7hwxjDgSPNbDhwlI0H6thw4CgPvrmbLv9FOKZ4E5ibm6q9eKVCRHvoKqSa2zvZXF7PhgNH2bC/jo0Hjg7Zi5+bm8rc3DTStRev1ElpD13ZxhPrYtHUDBZNta4Z3tOL7+nBay9eqeDRHrqyXSC9+J6Q1168Gu+0h9UcmAcAAAo8SURBVK7C2lC9+LIjLb09+A0HjvLQm3u0F6/UMDTQVdgREXIzPORmeLh27mRgYC9+44E6Vu6s5tkNFQB4Yp3MyU5lXp4V8mdMSiEzOU7XoFHjjga6iggj7cUnuV3MzExiRlYSMzOTKMhMZGZmEhm6Ho2KYjqGrqJGS3sX71fUs+PwMXZUNrDzcCM7Khuob+nobeNNjKVgQhIzs5KYkZnEzKxECjKTSNblglWEGNUYuojkAI8BmYABlhtjfjWozfnAX4G9/rueNcZ8fzRFKzVS8bFOFk5JZ+GU9N77jDFUN7Sxo7KBHYcb2FnZwI5K69J8ze1dve0mprj9AZ9EwYREZmYlMX1CIp5Y/SNWRY5A/rd2Al8zxmwQkSRgvYi8Zoz5YFC7VcaYK4NfolKnTkSYkOxmQrKbogJf7/3d3YaKuhZ/wDdQWtnIjsMNrNlTS3tnt/97ITfd4+/RJ/YG/hRvgq5bo8LSsIFujDkEHPLvN4jINmAyMDjQlYoYDoeQk+4hJ93DRadn9t7f2dXN/iPNlFY2sONwY2/gv7Gjqnd83ukQpngTrDH6zCRmZCYyIyuJvHQPLqfDrh9JqZF9KCoi+cBcYN0QhxeLyCbgIPB1Y8zWUVen1BhzOR1M8yUyzZfIpWf23d/W2cXemqbeYZudlY1sOVjPS1sO0fMxVKzLwXRfIjMyrXH5ab4EpngTycvw6IW41ZgI+ENREUkE3gR+ZIx5dtCxZKDbGNMoIpcDvzLGFAzxGEuBpQC5ubnz9+/fP9r6lbJVc3snu6oa2Vnp7837A/9QfWtvGxGYlBLPFG8CU7wJ5HsTmOrfz06L1169GpFRr7YoIjHAC8Arxph7Ami/Dyg0xtScqI3OclHRrKG1g301zeypaWRvTRP7aprYW9PEnpomGlo7e9u5HEJuuqc36Kf027KS3Tj0hCk1yGhnuQjwCLDtRGEuIllApTHGiMhCwAHUjqJmpSJakjuG2dkpzM5OGXC/MYYjTe294d4T9Htrmli9u4bWju7etu4YB/kZCUP27NMTYvXEKXWcQMbQlwC3AO+LyHv++74N5AIYYx4CbgDuEJFOoAW4ydg1wV2pMCYiZCTGkZEYR2F++oBj3d2Gw8da2ecP+56e/Y7DDbz2QSWd3X2/UkluV2+49/Tsp3oTyfd6SNI59eOWnlikVATo7Oqm/GhLb2++/3awvoX+v8bexDimehPI93rI9yaQl55Ann8pBT2BKvLp4lxKRTiX00G+vzd+waBjrR1dHDjSzJ7qpgHj9Su2V1PTWD6gbZonhtx0D7kZCeSlWyHf8zUzScfsI50GulIRzh3j9M+HTzruWGNbJwdqmzlwpIn9tc3sP9LMgdpmNpXV8dL7h3rn1gPEuRzkpFsBn5PuIS/D2nLTE8hJj9eTqSKABrpSUSwxzsWsScnMmpR83LGOrm4O1rX0Bn3ZkWb211rBv2ZP7YClEUQgK9lNbm/QJ1g9ff/tVI+uUR8ONNCVGqdinA7yMhLIy0g47pgxhtqmdvb3691bPf1m3thRTXXDwKGcZLerL+T7DePkZVjTL3W9+rGhga6UOo6I4E2Mw5sYx/y8tOOON7d3cuBIM/tre3r2Vi9/68F6Xtl6eMCMnFing8lp8WSnxTM51dqy0+OZnOphclq8Bn4QaaArpUbME+vitKxkTss6fiins6ubQ/WtvYG//0gT5UdaKK9rYdu2Smoa2we0dzmErBS3FfRpnt7wz06NZ3JaPBNT4ol16dm0gdBAV0oFlcvp6F34bMn044+3dnRRUddC+dEWKo62UFHX3Lv/9u4aDh9rHTANUwQyk9wDe/lp/vBPte7TtXIsGuhKqTHljnH2LoA2lPbObg7Xt1LeL+itN4BmNhw4youbDw0Y0gHrwiX9e/g9Qd+zP15OttJAV0qFlViXo/easkPp6jZUHmulos4K+/Kjzb09/m2HjvHatsreNe17pMTH9PbsJ6W4mZQaz8TUeCanWvsTkqJjHF8DXSkVUZwOYVJqPJNS41mQf/xxYww1je29QW+FvtXLLzvSzNo9tQMWSOt5zKxkNxP9YW9tbialxDMx1RrfT4mPCfv1czTQlVJRRUTwJcXhS4pjbu7xM3TAWg3zUL3Vyz9U18rBuhYO1rdwsK6F98rqeHnLYdq7BvbyPbHOvsBP6Rf6/jeAiSlu28fyNdCVUuNOkjuGJHfMkGfXgrVQWk1TW2/YV9S1cKi+J/hb2X64iuqGtuO+LyMhtjfcBwf+pJR4fElxIR3a0UBXSqlBHA5hQpKbCUlu5uSkDtmmrbOLyvo2f9i3+IO/lUP1LeyrbeLt3bU0tg0c2nE5hMxkN7edm89ni6cGvW4NdKWUOgVxLudJP7wFONbawUH/sE5f8LcyITkuJDVpoCulVIgku2NIzooZ8gSsUNDTr5RSKkpooCulVJTQQFdKqSihga6UUlFCA10ppaKEBrpSSkUJDXSllIoSGuhKKRUlxBgzfKtQPLFINbD/FL/dC9QEsZxIp6/HQPp69NHXYqBoeD3yjDG+oQ7YFuijISIlxphCu+sIF/p6DKSvRx99LQaK9tdDh1yUUipKaKArpVSUiNRAX253AWFGX4+B9PXoo6/FQFH9ekTkGLpSSqnjRWoPXSml1CAa6EopFSUiLtBF5FIR2SEiu0TkW3bXYycRyRGRN0TkAxHZKiJftrsmu4mIU0Q2isgLdtdiNxFJFZFnRGS7iGwTkcV212QXEfmK/3dki4j8SUTcdtcUChEV6CLiBB4ALgNmAR8TkVn2VmWrTuBrxphZwCLgC+P89QD4MrDN7iLCxK+Al40xpwFzGKevi4hMBu4ECo0xZwJO4CZ7qwqNiAp0YCGwyxizxxjTDjwBXGNzTbYxxhwyxmzw7zdg/cJOtrcq+4hINnAF8LDdtdhNRFKAYuARAGNMuzGmzt6qbOUC4kXEBXiAgzbXExKRFuiTgbJ+t8sZxwHWn4jkA3OBdfZWYqtfAt8Euu0uJAxMAaqB3/uHoB4WkQS7i7KDMaYC+DlwADgE1BtjXrW3qtCItEBXQxCRRODPwL8ZY47ZXY8dRORKoMoYs97uWsKEC5gHPGiMmQs0AePyMycRScP6S34KMAlIEJGb7a0qNCIt0CuAnH63s/33jVsiEoMV5o8bY561ux4bLQGuFpF9WENxF4rIH+0tyVblQLkxpucvtmewAn48+hCw1xhTbYzpAJ4FzrW5ppCItEB/FygQkSkiEov1wcbzNtdkGxERrDHSbcaYe+yux07GmLuMMdnGmHys/xcrjDFR2QsLhDHmMFAmIjP9d10EfGBjSXY6ACwSEY//d+YiovQDYpfdBYyEMaZTRL4IvIL1SfXvjDFbbS7LTkuAW4D3ReQ9/33fNsa8ZGNNKnx8CXjc3/nZA3zK5npsYYxZJyLPABuwZoZtJEqXANBT/5VSKkpE2pCLUkqpE9BAV0qpKKGBrpRSUUIDXSmlooQGulJKRQkNdKWUihIa6EopFSX+Py+FFC5b/v+yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst2 = MAML()\n",
    "tst2.train(1, num_tasks, num_grad_steps, meta_lr, step_lr)\n",
    "\n",
    "tasks = [SineTask() for t in range(20)]\n",
    "\n",
    "trained = []\n",
    "untrained = []\n",
    "\n",
    "for task in tasks:\n",
    "    # should get same data\n",
    "    ell1 = tst.fine_tune(task) \n",
    "    trained.append(ell1)\n",
    "    ell2 = tst2.fine_tune(task)\n",
    "    untrained.append(ell2)\n",
    "\n",
    "trained_agg = np.mean(trained, axis=0)\n",
    "untrained_agg = np.mean(untrained, axis=0)\n",
    "\n",
    "plt.plot(trained_agg)\n",
    "plt.plot(untrained_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5502])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'relasdfsadf1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relasdfsadf'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.1240]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.124039"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[2,3,6,4,8]])\n",
    "b = torch.Tensor([[1,9,8,9,8]])\n",
    "c = torch.cdist(a,b)\n",
    "\n",
    "dist = np.linalg.norm(a.numpy()-b.numpy())\n",
    "print(c)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+36+4+25+0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.12403840463596"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
