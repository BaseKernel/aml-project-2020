{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class SineTask:\n",
    "    def __init__(self):\n",
    "        self.amplitude = np.random.uniform(0.1, 5.0)\n",
    "        self.phase = np.random.uniform(0, 2*np.pi)\n",
    "        self.hold_x = None\n",
    "        \n",
    "    def sin(self, x):\n",
    "        return self.amplitude * np.sin(x + self.phase)\n",
    "    \n",
    "    def training_data(self, n=10, fresh = False):\n",
    "        if self.hold_x is None:\n",
    "            self.hold_x = np.random.uniform(-5, 5, size=(n,1))\n",
    "            x = self.hold_x\n",
    "        else:\n",
    "            if fresh:\n",
    "                x = np.random.uniform(-5, 5, size=(n,1))\n",
    "            else:\n",
    "                x = self.hold_x\n",
    "        y = self.sin(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "            \n",
    "        \n",
    "    def test_data(self, n=50):\n",
    "        x = np.linspace(-5, 5, num=n).reshape(n,1)\n",
    "        y = self.sin(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "\n",
    "    \n",
    "class SineModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(1, 40)\n",
    "        self.layer2 = torch.nn.Linear(40, 40)\n",
    "        self.layer3 = torch.nn.Linear(40, 1)\n",
    "    \n",
    "    def forward(self, x, parameters=None):\n",
    "        x = F.linear(x, parameters['layer1.weight'], parameters['layer1.bias'])\n",
    "        x = F.relu(x)\n",
    "        x = F.linear(x, parameters['layer2.weight'], parameters['layer2.bias'])\n",
    "        x = F.relu(x)\n",
    "        x = F.linear(x, parameters['layer3.weight'], parameters['layer3.bias'])\n",
    "        return x\n",
    "    \n",
    "    def copy(self, model):\n",
    "        self.load_state_dict(model.state_dict())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x, parameters=None):\n",
    "        #probably this if.\n",
    "        if parameters is None:\n",
    "            x = self.layer1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.layer2(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.layer3(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = F.linear(x, parameters['layer1.weight'], parameters['layer1.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.linear(x, parameters['layer2.weight'], parameters['layer2.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.linear(x, parameters['layer3.weight'], parameters['layer3.bias'])\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jo\n",
      "10.914102554321289\n",
      "Jo\n",
      "0.11748529970645905\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [40, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7e17fa56e6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmeta_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmeta_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [40, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "#hold_parameters = None \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# be carefull of not ImplementError setting in example code. probably a reason.\n",
    "\n",
    "model = SineModel()\n",
    "global_parameters = { name: par for name, par in model.named_parameters()}\n",
    "hold_parameters = global_parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.zero_grad()\n",
    "metric = torch.nn.MSELoss()\n",
    "history = []\n",
    "tasks = [SineTask() for m in range(10)]\n",
    "gradient_steps = 3\n",
    "lr_inner = 0.01\n",
    "\n",
    "for task in tasks:\n",
    "    task_model = SineModel()\n",
    "    task_model.copy(model)\n",
    "\n",
    "    for step in range(gradient_steps):\n",
    "        x, y = task.training_data()\n",
    "        y_hat = task_model.forward(x, hold_parameters)\n",
    "        loss = metric(y_hat, y)\n",
    "        grads = torch.autograd.grad(loss, hold_parameters.values(), create_graph = True )\n",
    "\n",
    "        # note that now you only use manual parameters, graph parameters not updated\n",
    "        hold_parameters = {name:(parameter - lr_inner * grad) for ((name, parameter), grad) in zip(hold_parameters.items(), grads)}\n",
    "\n",
    "    # task update\n",
    "    print('Jo')\n",
    "    x, y = task.training_data(fresh=True)\n",
    "    y_hat = model.forward(x, hold_parameters)\n",
    "    meta_loss = metric(y_hat, y)\n",
    "    print(meta_loss.item())\n",
    "    meta_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    history.append(loss.item())\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1.weight': tensor([[ 5.7750e-02],\n",
       "         [ 2.9777e-01],\n",
       "         [ 6.9563e-02],\n",
       "         [ 8.1815e-01],\n",
       "         [ 3.8379e-01],\n",
       "         [-2.9962e-01],\n",
       "         [ 6.9574e-01],\n",
       "         [ 4.8989e-04],\n",
       "         [ 8.0888e-01],\n",
       "         [-7.2096e-01],\n",
       "         [ 3.5985e-01],\n",
       "         [ 7.9879e-02],\n",
       "         [-5.0667e-01],\n",
       "         [ 5.0920e-01],\n",
       "         [ 8.2912e-01],\n",
       "         [-6.6041e-01],\n",
       "         [ 9.7944e-03],\n",
       "         [ 3.0080e-01],\n",
       "         [-8.0946e-01],\n",
       "         [ 9.4273e-01],\n",
       "         [ 5.1069e-01],\n",
       "         [ 5.8100e-01],\n",
       "         [-2.3837e-01],\n",
       "         [ 1.5779e-01],\n",
       "         [ 1.0778e-01],\n",
       "         [ 8.1755e-01],\n",
       "         [ 3.8943e-01],\n",
       "         [ 1.4335e-01],\n",
       "         [-2.7332e-01],\n",
       "         [-1.4913e-01],\n",
       "         [-8.0380e-01],\n",
       "         [-9.8003e-01],\n",
       "         [ 6.7918e-01],\n",
       "         [ 5.6595e-01],\n",
       "         [ 3.0601e-01],\n",
       "         [-3.9063e-01],\n",
       "         [-2.8660e-01],\n",
       "         [-5.0420e-01],\n",
       "         [ 6.2248e-01],\n",
       "         [ 9.8471e-01]], grad_fn=<SubBackward0>),\n",
       " 'layer1.bias': tensor([ 0.1220, -0.1152,  0.4005, -0.0854,  0.1159,  0.6287,  0.8881,  0.9285,\n",
       "         -0.7941, -0.1323,  0.5922, -0.2799, -0.4200,  0.9270,  0.4749, -0.2140,\n",
       "         -0.6969,  0.2400, -0.7048,  0.0251,  0.3323, -0.4230, -0.0189,  0.0688,\n",
       "          0.2981,  0.0838, -0.4695, -0.2441,  0.1634,  0.6449,  0.6760,  0.4670,\n",
       "          0.6815,  0.6246, -0.4546, -0.6936, -0.7243, -0.1178,  0.5189, -0.7918],\n",
       "        grad_fn=<SubBackward0>),\n",
       " 'layer2.weight': tensor([[ 0.0976,  0.1066,  0.1398,  ..., -0.0189, -0.1301,  0.1272],\n",
       "         [-0.0969,  0.1264,  0.0840,  ..., -0.1076, -0.0577,  0.1527],\n",
       "         [-0.1298, -0.0845,  0.1474,  ..., -0.0023, -0.0754,  0.0771],\n",
       "         ...,\n",
       "         [-0.0934,  0.1117,  0.1505,  ..., -0.0385,  0.1220,  0.1471],\n",
       "         [-0.0728,  0.0890, -0.1010,  ..., -0.0017, -0.0972,  0.0594],\n",
       "         [ 0.1233, -0.0983, -0.0869,  ..., -0.0631,  0.0382,  0.1133]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " 'layer2.bias': tensor([-0.1087,  0.0147, -0.1278, -0.1303, -0.1016,  0.0478,  0.1278, -0.0064,\n",
       "         -0.1333, -0.0069,  0.1510, -0.1189,  0.0446,  0.1042, -0.1315, -0.1346,\n",
       "         -0.0463, -0.1416,  0.0023, -0.0537, -0.0601,  0.0715,  0.1569, -0.0658,\n",
       "         -0.1422, -0.0752, -0.1560,  0.0862,  0.1356,  0.0067,  0.1265, -0.1075,\n",
       "          0.0071,  0.0814,  0.0426, -0.0756,  0.0786,  0.1091, -0.0055,  0.0529],\n",
       "        grad_fn=<SubBackward0>),\n",
       " 'layer3.weight': tensor([[ 0.0002,  0.1216, -0.0947,  0.1339, -0.0641,  0.0511, -0.1349, -0.1039,\n",
       "          -0.0279, -0.0653, -0.0985,  0.0250,  0.0631,  0.0098, -0.0239, -0.0260,\n",
       "          -0.0741,  0.0942,  0.0199, -0.0387, -0.0172,  0.0037,  0.0613,  0.1124,\n",
       "           0.0636,  0.1063,  0.1526,  0.0904,  0.1013, -0.0639,  0.0443, -0.0194,\n",
       "          -0.1088,  0.1575, -0.0863, -0.1402,  0.0188, -0.0427,  0.0947, -0.1417]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " 'layer3.bias': tensor([0.0355], grad_fn=<SubBackward0>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
