{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# (fixme) change the comments to own words. change variable names dim_input -> input_dim.\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "def flatten(parameters):\n",
    "    l = [torch.flatten(p) for p in parameters]\n",
    "    return torch.cat(l).view(-1,1)\n",
    "\n",
    "def unflatten(flat, size_ids):\n",
    "    D = [np.prod(E) for E in size_ids]\n",
    "    D.insert(0,0)\n",
    "    D = np.cumsum(D)\n",
    "    E = [(D[i],D[i+1]) for i in range(D.size-1)]\n",
    "    \n",
    "    l = [flat[s:e] for (s, e) in E]\n",
    "    for i, p in enumerate(size_ids):\n",
    "        l[i] = l[i].view(*p)\n",
    "    return l\n",
    "\n",
    "def hessian_matrix(grads, params):\n",
    "    components = sum([1 for param in grads])\n",
    "    size_id = [comp.shape for comp in grads]\n",
    "    m = sum([np.prod(p) for p in size_id])\n",
    "    hessian = torch.zeros(m,m)\n",
    "    \n",
    "    k = 0 \n",
    "    for i in range(components):\n",
    "        B = np.ndindex(grads[i].shape)\n",
    "        for j in B:\n",
    "            # calculates gradient of the gradient w.r.t all the parameters.\n",
    "            hessian[:,k]= flatten(torch.autograd.grad(grads[i][j], params, retain_graph=True)).reshape(m)\n",
    "            k+=1 \n",
    "    return hessian\n",
    "\n",
    "def hessian_vector_products(grad, hessians, lr):\n",
    "    m = hessians[0].shape[0]\n",
    "    I = torch.eye(m)\n",
    "    size_id = [comp.shape for comp in grad]\n",
    "    grad_flat = flatten(grad).reshape(m)\n",
    "    \n",
    "    for hes in reversed(hessians):\n",
    "        grad_flat = torch.mv(I - lr*hes, grad_flat)\n",
    "    return unflatten(grad_flat, size_id)\n",
    "\n",
    "def empty_gradient(params):\n",
    "    grad = []\n",
    "    for par in params:\n",
    "        grad.append(torch.zeros(par.shape))\n",
    "    return grad\n",
    "        \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 40)\n",
    "        self.layer2 = torch.nn.Linear(40,40)\n",
    "        self.layer3 = torch.nn.Linear(40, output_dim)\n",
    "        \n",
    "    def forward(self, x, weight=None):\n",
    "        if weight:\n",
    "            x = F.linear(x, weight[0], weight[1])\n",
    "            x = F.relu(x)\n",
    "            x = F.linear(x, weight[2], weight[3])\n",
    "            x = F.relu(x)\n",
    "            x = F.linear(x, weight[4], weight[5])\n",
    "        else:\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "            x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MAML():\n",
    "    def __init__(self, model, dim_input=1, dim_output=1, inner_lr=1e-2, outer_lr=1e-3, shots = 10, epochs = 100, task_updates=2, verbose=True):\n",
    "        self.input_dim = dim_input\n",
    "        self.output_dim = dim_output\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.shots = shots\n",
    "        self.epochs = epochs\n",
    "        self.task_updates = task_updates\n",
    "        self.meta_loss_history = []\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = model\n",
    "        self.measure = torch.nn.MSELoss()\n",
    "        self.weights = list(model.parameters())\n",
    "        self.meta_optimizer = torch.optim.Adam(self.weights, self.outer_lr)\n",
    "        \n",
    "    def inner(self, task, approx=False):\n",
    "        \n",
    "        if not approx:\n",
    "            hessians = []\n",
    "        \n",
    "        temp_weights = [w.clone() for w in self.weights]\n",
    "        batch = task[np.random.randint(task.shape[0], size = self.shots)]\n",
    "        \n",
    "        for step in range(self.task_updates):\n",
    "            #print(temp_weights[0])\n",
    "            loss = self.measure(self.model.forward(batch[:,:-1].float(), weight=temp_weights), batch[:,-1])\n",
    "            \n",
    "            # compute grad and hessian\n",
    "            grad = torch.autograd.grad(loss, temp_weights, retain_graph=True, create_graph=True)\n",
    "            if not approx:\n",
    "                hessians.append(hessian_matrix(grad, temp_weights))\n",
    "            # update the parameters\n",
    "            temp_weights = [w - self.inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "        \n",
    "        \n",
    "        batch = task[np.random.randint(task.shape[0], size = self.shots)]\n",
    "        loss = self.measure(self.model.forward(batch[:,:-1], weight=temp_weights), batch[:,-1])\n",
    "        hold_grad = torch.autograd.grad(loss, temp_weights)\n",
    "        if not approx:\n",
    "            task_grad = hessian_vector_products(hold_grad, hessians, self.inner_lr)\n",
    "        else:\n",
    "            task_grad = hold_grad\n",
    "        return task_grad, loss\n",
    "        \n",
    "\n",
    "    def outer(self, data_generator, epochs):\n",
    "        if epochs:\n",
    "            self.epochs = epochs\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            sup_x, sup_y, _, _ = data_generator.generate()\n",
    "            tasks = torch.tensor(np.concatenate((sup_x, sup_y), axis=2).astype(np.float32))\n",
    "            meta_grad = empty_gradient(self.weights)\n",
    "            meta_loss = 0\n",
    "            \n",
    "            # perform inner loop\n",
    "            # should be able to paralellize this\n",
    "            for t in range(data_generator.batch_size):\n",
    "                task_grad, task_loss = self.inner(tasks[t], approx=True)\n",
    "                meta_grad = [e1 + e2 for (e1, e2) in zip(meta_grad, task_grad)]\n",
    "                meta_loss += task_loss\n",
    "            \n",
    "            # assign meta gradiet to weights\n",
    "            #print(meta_grad)\n",
    "            \n",
    "            for weight, grad in zip(self.weights, meta_grad):\n",
    "                weight.grad = grad\n",
    "            self.meta_optimizer.step()\n",
    "            \n",
    "            # \n",
    "            epoch_loss += meta_loss / data_generator.batch_size\n",
    "            \n",
    "            #self.meta_optimizer.zero_grad()\n",
    "            if self.verbose:\n",
    "                if epoch % 50 == 0:\n",
    "                    print(\"{}/{}. loss: {}\".format(epoch, epochs, epoch_loss / 50 ))\n",
    "            \n",
    "                if epoch % 50 == 0:\n",
    "                    self.meta_loss_history.append(epoch_loss / 50)\n",
    "                    epoch_loss = 0\n",
    "        \n",
    "         \n",
    "def main():\n",
    "    \n",
    "    datasource = 'sinusoid'\n",
    "    update_batch_size = 10       # 'number of examples used for inner gradient update (K for K-shot learning).'\n",
    "    meta_batch_size = 500       # 'number of tasks sampled per meta-update'\n",
    "    \n",
    "    data_generator = DataGenerator('sinusoid', meta_batch_size, update_batch_size)\n",
    "    \n",
    "    dim_input = data_generator.dim_input\n",
    "    dim_output = data_generator.dim_output\n",
    "    \n",
    "    net = Net(dim_input, dim_output)\n",
    "    model = MAML(net, verbose=True)\n",
    "    support_x, support_y, _, _ = data_generator.generate()\n",
    "    support = torch.tensor(np.concatenate((support_x, support_y), axis=2).astype(np.float32))\n",
    "    model.outer(data_generator, 50)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def construct_fc_weights(self):\n",
    "    weights = {}\n",
    "    weights['w1'] = tf.Variable(tf.truncated_normal([self.dim_input, self.dim_hidden[0]], stddev=0.01))\n",
    "    weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden[0]]))\n",
    "    for i in range(1,len(self.dim_hidden)):\n",
    "        weights['w'+str(i+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[i-1], self.dim_hidden[i]], stddev=0.01))\n",
    "        weights['b'+str(i+1)] = tf.Variable(tf.zeros([self.dim_hidden[i]]))\n",
    "    weights['w'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[-1], self.dim_output], stddev=0.01))\n",
    "    weights['b'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.zeros([self.dim_output]))\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "def task_metalearn(inp, reuse=True):\n",
    "    \"\"\" Perform gradient descent for one task in the meta-batch. \"\"\"\n",
    "    inputa, inputb, labela, labelb = inp\n",
    "    task_outputbs, task_lossesb = [], []\n",
    "\n",
    "    task_outputa = self.forward(inputa, weights, reuse=reuse)  # only reuse on the first iter\n",
    "    task_lossa = self.loss_func(task_outputa, labela)\n",
    "    grads = tf.gradients(task_lossa, list(weights.values()))\n",
    "                \n",
    "    if FLAGS.stop_grad:\n",
    "        grads = [tf.stop_gradient(grad) for grad in grads]\n",
    "                \n",
    "    gradients = dict(zip(weights.keys(), grads))\n",
    "    fast_weights = dict(zip(weights.keys(), [weights[key] - self.update_lr*gradients[key] for key in weights.keys()]))\n",
    "    output = self.forward(inputb, fast_weights, reuse=True)\n",
    "    task_outputbs.append(output)\n",
    "    task_lossesb.append(self.loss_func(output, labelb))\n",
    "                \n",
    "    for j in range(num_updates - 1):\n",
    "        loss = self.loss_func(self.forward(inputa, fast_weights, reuse=True), labela)\n",
    "        grads = tf.gradients(loss, list(fast_weights.values()))\n",
    "        if FLAGS.stop_grad:\n",
    "            grads = [tf.stop_gradient(grad) for grad in grads]\n",
    "        gradients = dict(zip(fast_weights.keys(), grads))\n",
    "        fast_weights = dict(zip(fast_weights.keys(), [fast_weights[key] - self.update_lr*gradients[key] for key in fast_weights.keys()]))\n",
    "        output = self.forward(inputb, fast_weights, reuse=True)\n",
    "        task_outputbs.append(output)\n",
    "        task_lossesb.append(self.loss_func(output, labelb))\n",
    "\n",
    "                task_output = [task_outputa, task_outputbs, task_lossa, task_lossesb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
